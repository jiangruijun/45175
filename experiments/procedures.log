Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 5.4.0-42-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

New release '22.04.3 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Last login: Sun Aug 27 07:45:27 2023 from 10.255.255.1
mininet@mininet-vm:~$ cd mininet/mininet/examples/2809210j/
mininet@mininet-vm:~/mininet/mininet/examples/2809210j$ sudo python experiments.pmininet@mininet-vm:~/mininet/mininet/examples/2809210j$ sudo python experiments.py 
######################################################################
# This script was developed by Ruijun Jiang for
# his final project (MSc) at the University of Glasgow
######################################################################

######################################################################
# (0) Erase any remaining configurations, and
#     reset the Mininet environment
######################################################################
*** Removing excess controllers/ofprotocols/ofdatapaths/pings/noxes
killall controller ofprotocol ofdatapath ping nox_corelt-nox_core ovs-openflowd ovs-controllerovs-testcontroller udpbwtest mnexec ivs ryu-manager 2> /dev/null
killall -9 controller ofprotocol ofdatapath ping nox_corelt-nox_core ovs-openflowd ovs-controllerovs-testcontroller udpbwtest mnexec ivs ryu-manager 2> /dev/null
pkill -9 -f "sudo mnexec"
*** Removing junk from /tmp
rm -f /tmp/vconn* /tmp/vlogs* /tmp/*.out /tmp/*.log
*** Removing old X11 tunnels
*** Removing excess kernel datapaths
ps ax | egrep -o 'dp[0-9]+' | sed 's/dp/nl:/'
***  Removing OVS datapaths
ovs-vsctl --timeout=1 list-br
ovs-vsctl --timeout=1 list-br
*** Removing all links of the pattern foo-ethX
ip link show | egrep -o '([-_.[:alnum:]]+-eth[[:digit:]]+)'
ip link show
*** Killing stale mininet node processes
pkill -9 -f mininet:
*** Shutting down stale tunnels
pkill -9 -f Tunnel=Ethernet
pkill -9 -f .ssh/mn
rm -f ~/.ssh/mn/*
*** Cleanup complete.
######################################################################
# (1) Create and run the testing environment
#
# h1-----|                                              |-----h2
#        |----------s1----------s2----------s3----------|
# h3-----|                                              |-----h4
#
######################################################################
*** Adding controller
*** Add switches
*** Add hosts
*** Add links
*** Starting network
*** Configuring hosts
h1 h2 h3 h4 
*** Starting controllers
*** Starting switches
*** Post configure switches and hosts
######################################################################
# (2) Configure the testing environment
#
#     (2.1) Modify receive & send buffer size limits on all the hosts
#           [20 * BDP]
######################################################################
*** Modifying buffers on all the hosts
######################################################################
#     (2.2) Emulate a WAN with delay
#
#           (2.2.1) Add delay on the link between s1-s2
#                   [30 ms]
#           (2.2.2) Limit the link speed between s2-s3 to 1 Gbps, and
#                   modify the buffer size on the network node s2
#                   [10 * BDP]
#           (2.2.3) Deploy FQ-CoDel on the network node s2 egress port
######################################################################
*** Adding delay in the WAN
*** Limiting the speed of the bottleneck link
*** Deploying FQ-CoDel Active Queue Management
######################################################################
# (3) Conduct scenario 1 experiment 1: single flow [CUBIC]
#
#     (3.1) Deploy CUBIC on the sender host h1
#     (3.2) Run iPerf3 test (h1-h2)
######################################################################
*** Deploying CUBIC TCP congestion control
*** Executing ping to avoid ARP bias...
*** Ping: testing ping reachability
h1 -> h2 h3 h4 
h2 -> h1 h3 h4 
h3 -> h1 h2 h4 
h4 -> h1 h2 h3 
*** Results: 0% dropped (12/12 received)
*** Starting iPerf3 server on h2...
*** Waiting for iPerf3 server to start...
*** Running iPerf3 client on h1...
*** Results saved!
*** Stoping iPerf3 server on h2...
######################################################################
# (4) Conduct scenario 1 experiment 2: single flow [BBR]
#
#     (4.1) Deploy BBR on the sender host h1
#     (4.2) Run iPerf3 test (h1-h2)
######################################################################
*** Deploying BBR TCP congestion control
*** Executing ping to avoid ARP bias...
*** Ping: testing ping reachability
h1 -> h2 h3 h4 
h2 -> h1 h3 h4 
h3 -> h1 h2 h4 
h4 -> h1 h2 h3 
*** Results: 0% dropped (12/12 received)
*** Starting iPerf3 server on h2...
*** Waiting for iPerf3 server to start...
*** Running iPerf3 client on h1...
*** Results saved!
*** Stoping iPerf3 server on h2...
######################################################################
# (5) Conduct scenario 2 experiment 1: 2 parallel flows from 1 host
#                                      [CUBIC]
#
#     (5.1) Deploy CUBIC on the sender host h1
#     (5.2) Run iPerf3 test (h1-h2)
######################################################################
*** Deploying CUBIC TCP congestion control
*** Executing ping to avoid ARP bias...
*** Ping: testing ping reachability
h1 -> h2 h3 h4 
h2 -> h1 h3 h4 
h3 -> h1 h2 h4 
h4 -> h1 h2 h3 
*** Results: 0% dropped (12/12 received)
*** Starting iPerf3 server on h2...
*** Waiting for iPerf3 server to start...
*** Running iPerf3 client on h1...
*** Results saved!
*** Stoping iPerf3 server on h2...
######################################################################
# (6) Conduct scenario 2 experiment 2: 2 parallel flows from 1 host
#                                      [BBR]
#
#     (6.1) Deploy BBR on the sender host h1
#     (6.2) Run iPerf3 test (h1-h2)
######################################################################
*** Deploying BBR TCP congestion control
*** Executing ping to avoid ARP bias...
*** Ping: testing ping reachability
h1 -> h2 h3 h4 
h2 -> h1 h3 h4 
h3 -> h1 h2 h4 
h4 -> h1 h2 h3 
*** Results: 0% dropped (12/12 received)
*** Starting iPerf3 server on h2...
*** Waiting for iPerf3 server to start...
*** Running iPerf3 client on h1...
*** Results saved!
*** Stoping iPerf3 server on h2...
######################################################################
# (7) Conduct scenario 3 experiment 1: 2 competing flows from 2 hosts
#                                      [2 CUBIC flows]
#
#     (7.1) Deploy CUBIC on the sender host h1
#     (7.2) Deploy CUBIC on the sender host h3
#     (7.3) Run iPerf3 test (h1-h2 & h3-h4)
######################################################################
*** Deploying CUBIC TCP congestion control
*** Deploying CUBIC TCP congestion control
*** Executing ping to avoid ARP bias...
*** Ping: testing ping reachability
h1 -> h2 h3 h4 
h2 -> h1 h3 h4 
h3 -> h1 h2 h4 
h4 -> h1 h2 h3 
*** Results: 0% dropped (12/12 received)
*** Starting iPerf3 server on h2...
*** Starting iPerf3 server on h4...
*** Waiting for iPerf3 server to start...
*** Running iPerf3 client on h1 and h3 simultaneously...
*** Results saved!
*** Stoping iPerf3 server on h2...
*** Stoping iPerf3 server on h4...
######################################################################
# (8) Conduct scenario 3 experiment 2: 2 competing flows from 2 hosts
#                                      [2 BBR flows]
#
#     (8.1) Deploy BBR on the sender host h1
#     (8.2) Deploy BBR on the sender host h3
#     (8.3) Run iPerf3 test (h1-h2 & h3-h4)
######################################################################
*** Deploying BBR TCP congestion control
*** Deploying BBR TCP congestion control
*** Executing ping to avoid ARP bias...
*** Ping: testing ping reachability
h1 -> h2 h3 h4 
h2 -> h1 h3 h4 
h3 -> h1 h2 h4 
h4 -> h1 h2 h3 
*** Results: 0% dropped (12/12 received)
*** Starting iPerf3 server on h2...
*** Starting iPerf3 server on h4...
*** Waiting for iPerf3 server to start...
*** Running iPerf3 client on h1 and h3 simultaneously...
*** Results saved!
*** Stoping iPerf3 server on h2...
*** Stoping iPerf3 server on h4...
######################################################################
# (9) Conduct scenario 3 experiment 3: 2 competing flows from 2 hosts
#                                      [1 CUBIC flow, 1 BBR flow]
#
#     (9.1) Deploy CUBIC on the sender host h1
#     (9.2) Deploy BBR on the sender host h3
#     (9.3) Run iPerf3 test (h1-h2 & h3-h4)
######################################################################
*** Deploying CUBIC TCP congestion control
*** Deploying BBR TCP congestion control
*** Executing ping to avoid ARP bias...
*** Ping: testing ping reachability
h1 -> h2 h3 h4 
h2 -> h1 h3 h4 
h3 -> h1 h2 h4 
h4 -> h1 h2 h3 
*** Results: 0% dropped (12/12 received)
*** Starting iPerf3 server on h2...
*** Starting iPerf3 server on h4...
*** Waiting for iPerf3 server to start...
*** Running iPerf3 client on h1 and h3 simultaneously...
*** Results saved!
*** Stoping iPerf3 server on h2...
*** Stoping iPerf3 server on h4...
######################################################################
# Note that: All experiments are completed!
#
# To proceed with further testing, enter the desired commands manually
# To exit, type "quit"
######################################################################

*** Starting CLI:
mininet> 
mininet> 
mininet> quit
*** Stopping 0 controllers

*** Stopping 6 links
......
*** Stopping 3 switches
s1 s2 s3 
*** Stopping 4 hosts
h1 h2 h3 h4 
*** Done
mininet@mininet-vm:~/mininet/mininet/examples/2809210j$ 
mininet@mininet-vm:~/mininet/mininet/examples/2809210j$ 
